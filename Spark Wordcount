# Enter the spark environment
pyspark

from pyspark import SparkContext
from pyspark.sql import SparkSession, Row

#Initialize a Spark context and session
sc = SparkContext(appName="stopwords")
spark = SparkSession.builder.appName("stopwords").getOrCreate()

# Load the TSV file into an RDD
rdd = sc.textFile("/user/hadoop/amazon_reviews_us_Camera_v1_00.tsv")

# Get the header and filter it out
header = rdd.first()
rdd_split_no_header = rdd.filter(lambda line: line != header).map(lambda line: line.split("\t"))

# Convert the RDD to a DataFrame
# Assuming the 14th column is the review text (index 13)
reviews_rdd = rdd_split_no_header.map(lambda fields: Row(reviewText=fields[13]))
reviews_df = spark.createDataFrame(reviews_rdd)

# Show the first few reviews before removing stopwords
print("Original Reviews:")
reviews_df.show(5, truncate=False)

# Removal of Stopwords
from pyspark.sql.functions import col, split
from pyspark.ml.feature import StopWordsRemover

# List of custom stopwords
custom_stopwords = ["As", "as", "others", "Junk", "be", "leave", "believe", "me", "a", "all", "get", "to", "you", "<br", "/>", "br>", "in", "I", "my", "did", "quite", "not", "was", "one", "second", "third", 
"for", "very", "should", "at", "use", "and", "it", "it.", "is", "take", "give", "just", "no", "the", "they", "came", "I'm", "of", "stuff", "don't", "will", "or", "seems", "our", "has", "when", "this", "too",
"we", "have", "had", "camera", "than", "are", "keeps", "added", "how", "its", "much", "out", "-", "little", "time", "/><br", "also"] 

# Tokenize the review texts into words
tokenized_reviews = reviews_df.withColumn("words", split(col("reviewText"), " "))

# Initialize StopWordsRemover with default and custom stopwords
stopwords_remover = StopWordsRemover(inputCol="words", outputCol="filteredWords")
default_stopwords = stopwords_remover.getStopWords()
combined_stopwords = default_stopwords + custom_stopwords
stopwords_remover = stopwords_remover.setStopWords(combined_stopwords)

# Remove stopwords
clean_reviews_df = stopwords_remover.transform(tokenized_reviews)

# Show the results
print("Reviews after stopwords removal:")
clean_reviews_df.select("filteredWords").show(truncate=False)

# Perform wordcount analysis
from pyspark.sql.functions import explode, count

# Explode the words column and count the occurrences
word_counts_df = clean_reviews_df.withColumn("word", explode(col("filteredWords"))) \
                .groupBy("word") \
                .agg(count("word").alias("count")) \
                .orderBy(col("count").desc())

# Show the most frequent words
print("Most frequent words:")
word_counts_df.show(20, truncate=False)

#Stop SparkContext and SparkSession
sc.stop()
spark.stop()
